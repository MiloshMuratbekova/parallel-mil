КОНТРОЛЬНЫЕ ВОПРОСЫ К ASSIGNMENT 2
(OpenMP, CUDA и гетерогенные вычисления)
====================================


ВОПРОС 1: Что понимается под гетерогенной параллелизацией?
==========================================================

Гетерогенная параллелизация - это подход к параллельным вычислениям, при котором
одновременно используются разные типы вычислительных устройств (процессоров)
для решения одной задачи.

Основная идея:
- Разные части программы выполняются на разных устройствах
- Каждое устройство делает ту работу, для которой оно лучше подходит
- CPU и GPU работают вместе, дополняя друг друга

Пример:
- CPU загружает данные и управляет процессом
- GPU выполняет массивные вычисления над данными
- CPU обрабатывает результат

Слово "гетерогенный" означает "разнородный" - то есть система состоит из
разных компонентов, а не из одинаковых (однородных/гомогенных).


ВОПРОС 2: В чём принципиальные различия архитектур CPU и GPU?
=============================================================

                        CPU                         GPU
                        ---                         ---
Количество ядер:        Мало (4-32)                 Много (сотни-тысячи)
Скорость ядра:          Высокая (3-5 ГГц)           Низкая (1-2 ГГц)
Размер кэша:            Большой (МБ)                Маленький (КБ на ядро)
Тип операций:           Сложные, разные             Простые, одинаковые
Управление потоками:    Независимое                 Группами (warps)
Переключение задач:     Быстрое                     Медленное
Ветвления (if/else):    Эффективные                 Неэффективные
Доступ к памяти:        Случайный - быстро          Случайный - медленно
Пропускная память:      ~50 ГБ/с                    ~500+ ГБ/с

Главное различие:

CPU - "умный, но один":
- Несколько мощных ядер
- Каждое ядро может делать сложную работу независимо
- Хорошо предсказывает переходы, умеет спекулятивно выполнять код
- Оптимизирован для последовательного выполнения

GPU - "много простых рабочих":
- Тысячи простых ядер
- Все ядра делают одно и то же над разными данными (SIMD/SIMT)
- Оптимизирован для массового параллелизма
- Высокая пропускная способность памяти


ВОПРОС 3: Какие типы задач лучше подходят для выполнения на GPU, а какие — на CPU?
==================================================================================

ЛУЧШЕ ДЛЯ GPU:
--------------
1. Обработка изображений и видео
   - Каждый пиксель обрабатывается независимо
   - Миллионы одинаковых операций

2. Матричные вычисления
   - Умножение матриц
   - Нейронные сети (много перемножений)

3. Физические симуляции
   - Расчет движения частиц
   - Каждая частица считается отдельно

4. Криптография и хеширование
   - Много попыток с разными данными

5. Научные вычисления
   - Численное решение уравнений
   - Моделирование

Характеристики GPU-задач:
✓ Много данных
✓ Одинаковые операции над каждым элементом
✓ Мало ветвлений (if/else)
✓ Независимые вычисления (нет зависимостей между элементами)


ЛУЧШЕ ДЛЯ CPU:
--------------
1. Последовательная логика
   - Алгоритмы с зависимостями между шагами
   - Рекурсия с изменяемым состоянием

2. Сложные ветвления
   - Много условий if/else
   - Разная логика для разных случаев

3. Ввод-вывод
   - Работа с файлами
   - Сетевые операции

4. Операционные системы и управление
   - Планирование задач
   - Управление памятью

5. Небольшие задачи
   - Когда накладные расходы на GPU больше выгоды

Характеристики CPU-задач:
✓ Сложная логика
✓ Много условных переходов
✓ Зависимости между вычислениями
✓ Небольшой объем данных
✓ Работа с внешними устройствами


ВОПРОС 4: Почему не все алгоритмы эффективно распараллеливаются с использованием OpenMP?
========================================================================================

Причины:

1. ЗАВИСИМОСТИ МЕЖДУ ИТЕРАЦИЯМИ
   Если результат итерации i нужен для итерации i+1, нельзя выполнять их
   параллельно.

   Пример - числа Фибоначчи:
   ```cpp
   fib[0] = 1; fib[1] = 1;
   for (int i = 2; i < n; i++) {
       fib[i] = fib[i-1] + fib[i-2];  // Зависит от предыдущих!
   }
   ```
   Здесь нельзя использовать #pragma omp parallel for

2. НАКЛАДНЫЕ РАСХОДЫ (OVERHEAD)
   - Создание и уничтожение потоков требует времени
   - Синхронизация потоков (barriers, critical sections)
   - Для маленьких задач overhead > выгода

3. КОНКУРЕНЦИЯ ЗА РЕСУРСЫ (CONTENTION)
   - Много потоков пишут в одну переменную
   - Critical section становится узким местом

   Пример из нашей сортировки выбором:
   ```cpp
   #pragma omp critical
   {
       if (local_min < global_min) {
           global_min = local_min;  // Только один поток за раз!
       }
   }
   ```

4. НЕРАВНОМЕРНАЯ НАГРУЗКА (LOAD IMBALANCE)
   - Разные итерации требуют разного времени
   - Быстрые потоки ждут медленные

5. ЛОЖНОЕ РАЗДЕЛЕНИЕ КЭША (FALSE SHARING)
   - Разные потоки работают с соседними элементами массива
   - Они попадают в одну строку кэша
   - Процессор постоянно синхронизирует кэш между ядрами

6. АЛГОРИТМИЧЕСКАЯ СЛОЖНОСТЬ
   - Некоторые алгоритмы inherently sequential (изначально последовательные)
   - Например: некоторые графовые алгоритмы, динамическое программирование


ВОПРОС 5: В чём заключается основная идея алгоритма сортировки слиянием?
========================================================================

Сортировка слиянием (Merge Sort) - это алгоритм "разделяй и властвуй" (divide and conquer).

ОСНОВНАЯ ИДЕЯ:
1. РАЗДЕЛЕНИЕ (Divide):
   - Делим массив пополам
   - Повторяем, пока не получим массивы из 1 элемента

2. СОРТИРОВКА (Conquer):
   - Массив из 1 элемента уже отсортирован

3. СЛИЯНИЕ (Merge):
   - Сливаем два отсортированных массива в один
   - Сравниваем элементы и берем меньший

Пример:
```
[38, 27, 43, 3, 9, 82, 10]

Разделение:
[38, 27, 43, 3]  |  [9, 82, 10]
[38, 27] [43, 3] |  [9, 82] [10]
[38] [27] [43] [3] [9] [82] [10]

Слияние:
[27, 38] [3, 43]  |  [9, 82] [10]
[3, 27, 38, 43]   |  [9, 10, 82]
[3, 9, 10, 27, 38, 43, 82]
```

Процедура слияния двух отсортированных массивов:
```
A = [3, 27]    B = [9, 43]    Result = []

Шаг 1: 3 < 9  → Result = [3]
Шаг 2: 27 > 9 → Result = [3, 9]
Шаг 3: 27 < 43 → Result = [3, 9, 27]
Шаг 4: остался 43 → Result = [3, 9, 27, 43]
```

Почему это хорошо для параллелизации:
- Подмассивы независимы - можно сортировать параллельно
- Слияние пар тоже можно делать параллельно
- Сложность O(n log n) в любом случае


ВОПРОС 6: Какие сложности возникают при реализации сортировки слиянием на GPU?
==============================================================================

1. НЕРЕГУЛЯРНЫЙ ДОСТУП К ПАМЯТИ
   - При слиянии нужно читать из двух разных мест
   - GPU любит последовательный доступ (coalesced memory access)
   - Случайный доступ сильно снижает производительность

2. ДИСБАЛАНС НАГРУЗКИ
   - На последних этапах слияния работы мало, но она важная
   - Сначала много маленьких слияний (хорошо для GPU)
   - Потом мало больших слияний (плохо для GPU)

   Пример: финальное слияние двух половин - только один блок работает

3. СИНХРОНИЗАЦИЯ МЕЖДУ ЭТАПАМИ
   - После каждого этапа слияния нужно синхронизировать все потоки
   - cudaDeviceSynchronize() создает паузу
   - Нельзя начать следующее слияние, пока не закончено предыдущее

4. НАКЛАДНЫЕ РАСХОДЫ НА КОПИРОВАНИЕ
   - Данные нужно копировать CPU → GPU в начале
   - И GPU → CPU в конце
   - Для небольших массивов это может занять больше времени, чем сортировка

5. ИСПОЛЬЗОВАНИЕ ДОПОЛНИТЕЛЬНОЙ ПАМЯТИ
   - Нужен временный буфер для слияния
   - На GPU память ограничена
   - Нужно переключаться между буферами (double buffering)

6. ПРОБЛЕМА ВЕТВЛЕНИЙ
   - Слияние содержит условия (if arr[i] < arr[j])
   - Разные потоки в warp могут пойти разными путями
   - Это вызывает thread divergence и снижает производительность

7. ВЫБОР РАЗМЕРА ПОДМАССИВОВ
   - Слишком маленькие: много overhead на запуск kernel'ов
   - Слишком большие: не помещаются в shared memory

8. КООРДИНАЦИЯ МЕЖДУ БЛОКАМИ
   - Блоки GPU не могут напрямую общаться
   - Нужно использовать глобальную память
   - Это медленно


ВОПРОС 7: Как выбор размера блока и сетки влияет на производительность вычислений на GPU?
=========================================================================================

РАЗМЕР БЛОКА (число потоков в блоке):
-------------------------------------

Слишком маленький блок (например, 32 потока):
✗ Недостаточно потоков для скрытия латентности памяти
✗ Низкая occupancy (заполненность SM)
✗ Плохое использование ресурсов

Слишком большой блок (например, 1024 потока):
✗ Может не хватить регистров на поток
✗ Может не хватить shared memory
✗ Меньше блоков может работать одновременно

Оптимальный размер (обычно 128-256 потоков):
✓ Достаточно для скрытия латентности
✓ Хорошая occupancy
✓ Кратен размеру warp (32 потока)

РАЗМЕР СЕТКИ (число блоков):
----------------------------

Слишком мало блоков:
✗ Не все SM (streaming multiprocessors) заняты
✗ GPU работает не на полную мощность

Слишком много блоков:
✗ Накладные расходы на управление
✗ Обычно не проблема, GPU хорошо справляется

Оптимально:
✓ Число блоков >= число SM × 2 (для скрытия латентности)
✓ Каждый блок должен иметь достаточно работы

ПРАКТИЧЕСКИЕ РЕКОМЕНДАЦИИ:
--------------------------

1. Размер блока должен быть кратен 32 (размер warp)
   Хорошо: 64, 128, 256, 512
   Плохо: 100, 200, 300

2. Минимум 2-4 блока на каждый SM для скрытия латентности

3. Использовать CUDA Occupancy Calculator для оптимизации

4. Учитывать ограничения:
   - Максимум 1024 потока на блок
   - Ограниченное количество регистров
   - Ограниченная shared memory

Пример из нашего кода:
```cpp
int threadsPerBlock = 256;  // Хорошее значение по умолчанию
int numBlocks = (n + threadsPerBlock - 1) / threadsPerBlock;
kernel<<<numBlocks, threadsPerBlock>>>(...)
```


ВОПРОС 8: Почему гетерогенный подход может быть эффективнее использования только CPU или только GPU?
====================================================================================================

ПРИЧИНЫ ЭФФЕКТИВНОСТИ ГЕТЕРОГЕННОГО ПОДХОДА:

1. ИСПОЛЬЗОВАНИЕ СИЛЬНЫХ СТОРОН КАЖДОГО УСТРОЙСТВА

   CPU хорош для:
   - Управления программой
   - Сложной логики и ветвлений
   - Работы с файлами и сетью
   - Небольших вычислений

   GPU хорош для:
   - Массивных параллельных вычислений
   - Обработки больших массивов данных
   - Простых одинаковых операций

   Гетерогенный подход: каждый делает то, что умеет лучше

2. ПАРАЛЛЕЛЬНАЯ РАБОТА CPU И GPU

   Пока GPU считает, CPU может:
   - Подготавливать следующую порцию данных
   - Обрабатывать результаты предыдущих вычислений
   - Выполнять другие задачи

   Это называется "overlapping" - перекрытие вычислений

3. ИЗБЕЖАНИЕ УЗКИХ МЕСТ

   Только CPU:
   ✗ Не хватает вычислительной мощности для больших данных

   Только GPU:
   ✗ Неэффективен для последовательных задач
   ✗ Накладные расходы на копирование данных
   ✗ Плохо справляется со сложной логикой

   Гетерогенный:
   ✓ CPU готовит данные, пока GPU считает
   ✓ GPU обрабатывает массивы, CPU - логику

4. РЕАЛЬНЫЙ ПРИМЕР - МАШИННОЕ ОБУЧЕНИЕ:

   Этап               Устройство    Причина
   ------             ----------    -------
   Загрузка данных    CPU           I/O операции
   Предобработка      CPU/GPU       Зависит от задачи
   Обучение модели    GPU           Матричные операции
   Валидация          GPU           Быстрые вычисления
   Сохранение модели  CPU           I/O операции
   Логирование        CPU           Текстовые операции

5. ЭКОНОМИЧЕСКАЯ ЭФФЕКТИВНОСТЬ

   - GPU дорогие и энергоемкие
   - Не имеет смысла использовать GPU для I/O или логики
   - Гетерогенный подход экономит деньги и энергию

6. МАСШТАБИРУЕМОСТЬ

   - Можно добавлять GPU к системе
   - CPU координирует работу нескольких GPU
   - Легче масштабировать, чем pure-GPU системы

ВЫВОД:
------
Гетерогенный подход эффективнее, потому что:
1. Каждое устройство делает то, для чего оно оптимизировано
2. Устройства работают параллельно, не простаивая
3. Избегаются слабые стороны каждого типа процессора
4. Лучшее соотношение производительность/стоимость

Это как команда из специалистов: программист не должен делать работу
дизайнера, а дизайнер - работу программиста. Каждый делает свое дело,
и вместе они работают эффективнее.
